message
"Collecting de-core-news-sm@ https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (from -r /opt/ml/model/code/requirements.txt (line 17))
  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 32.8 MB/s eta 0:00:00"
"Collecting gensim==4.3.1 (from -r /opt/ml/model/code/requirements.txt (line 1))
  Downloading gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/26.4 MB 25.6 MB/s eta 0:00:00"
"Collecting joblib==1.2.0 (from -r /opt/ml/model/code/requirements.txt (line 2))
  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 13.1 MB/s eta 0:00:00"
"Collecting keras==2.7.0 (from -r /opt/ml/model/code/requirements.txt (line 3))
  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 19.1 MB/s eta 0:00:00"
"Collecting Keras-Preprocessing==1.1.2 (from -r /opt/ml/model/code/requirements.txt (line 4))
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 2.0 MB/s eta 0:00:00"
"Collecting lightning-utilities==0.8.0 (from -r /opt/ml/model/code/requirements.txt (line 5))
  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)"
"Collecting matplotlib==3.7.1 (from -r /opt/ml/model/code/requirements.txt (line 6))
  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 45.0 MB/s eta 0:00:00"
Requirement already satisfied: numpy==1.22.4 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 7)) (1.22.4)
"Collecting pandas==1.3.5 (from -r /opt/ml/model/code/requirements.txt (line 8))
  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 46.7 MB/s eta 0:00:00"
"Collecting pytorch-crf==0.7.2 (from -r /opt/ml/model/code/requirements.txt (line 9))
  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)"
"Collecting pytorch-lightning==1.9.4 (from -r /opt/ml/model/code/requirements.txt (line 10))
  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 827.8/827.8 kB 13.8 MB/s eta 0:00:00"
"Collecting scikit-learn==1.2.2 (from -r /opt/ml/model/code/requirements.txt (line 11))
  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 53.1 MB/s eta 0:00:00"
"Collecting scipy==1.10.1 (from -r /opt/ml/model/code/requirements.txt (line 12))
  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 22.7 MB/s eta 0:00:00"
"Collecting SoMaJo==2.2.3 (from -r /opt/ml/model/code/requirements.txt (line 13))
  Downloading SoMaJo-2.2.3-py3-none-any.whl (91 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91.2/91.2 kB 1.9 MB/s eta 0:00:00"
"Collecting spacy==3.5.1 (from -r /opt/ml/model/code/requirements.txt (line 14))
  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 52.2 MB/s eta 0:00:00"
"Collecting spacy-legacy==3.0.12 (from -r /opt/ml/model/code/requirements.txt (line 15))
  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)"
"Collecting spacy-loggers==1.0.4 (from -r /opt/ml/model/code/requirements.txt (line 16))
  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)"
"Collecting sqlitedict==2.1.0 (from -r /opt/ml/model/code/requirements.txt (line 18))
  Downloading sqlitedict-2.1.0.tar.gz (21 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'"
"Collecting tensorflow-estimator==2.7.0 (from -r /opt/ml/model/code/requirements.txt (line 19))
  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 463.1/463.1 kB 9.8 MB/s eta 0:00:00"
"Collecting tensorflow-io-gcs-filesystem==0.31.0 (from -r /opt/ml/model/code/requirements.txt (line 20))
  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 33.0 MB/s eta 0:00:00"
"Collecting tokenizers==0.12.1 (from -r /opt/ml/model/code/requirements.txt (line 21))
  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 44.4 MB/s eta 0:00:00"
"Collecting torch==2.0.0 (from -r /opt/ml/model/code/requirements.txt (line 22))
  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 1.5 MB/s eta 0:00:00"
"Collecting torchaudio==2.0.1 (from -r /opt/ml/model/code/requirements.txt (line 23))
  Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 59.0 MB/s eta 0:00:00"
"Collecting torchmetrics==0.11.4 (from -r /opt/ml/model/code/requirements.txt (line 24))
  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.2/519.2 kB 12.6 MB/s eta 0:00:00"
"Collecting torchvision==0.15.1 (from -r /opt/ml/model/code/requirements.txt (line 25))
  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 64.1 MB/s eta 0:00:00"
"Collecting tqdm==4.62.3 (from -r /opt/ml/model/code/requirements.txt (line 26))
  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.2/76.2 kB 3.0 MB/s eta 0:00:00"
"Collecting transformers==4.27.2 (from -r /opt/ml/model/code/requirements.txt (line 27))
  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 50.2 MB/s eta 0:00:00"
"Collecting smart-open>=1.8.1 (from gensim==4.3.1->-r /opt/ml/model/code/requirements.txt (line 1))
  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata
  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)"
Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from Keras-Preprocessing==1.1.2->-r /opt/ml/model/code/requirements.txt (line 4)) (1.16.0)
Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from lightning-utilities==0.8.0->-r /opt/ml/model/code/requirements.txt (line 5)) (23.1)
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities==0.8.0->-r /opt/ml/model/code/requirements.txt (line 5)) (4.7.1)
Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (1.1.0)
Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (4.42.1)
Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (1.4.5)
Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (10.0.0)
Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (3.0.9)
Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1->-r /opt/ml/model/code/requirements.txt (line 6)) (2.8.2)
Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.3.5->-r /opt/ml/model/code/requirements.txt (line 8)) (2023.3.post1)
Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10)) (6.0)
"Collecting fsspec[http]>2021.06.0 (from pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Obtaining dependency information for fsspec[http]>2021.06.0 from https://files.pythonhosted.org/packages/6a/af/c673e8c663e17bd4fb201a6f029153ad5d7023aa4442d81c7987743db379/fsspec-2023.9.1-py3-none-any.whl.metadata
  Downloading fsspec-2023.9.1-py3-none-any.whl.metadata (6.7 kB)"
Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->-r /opt/ml/model/code/requirements.txt (line 11)) (3.2.0)
"Collecting regex>=2019.02.18 (from SoMaJo==2.2.3->-r /opt/ml/model/code/requirements.txt (line 13))
  Obtaining dependency information for regex>=2019.02.18 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 1.3 MB/s eta 0:00:00"
"Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/a8/ca/359ae4246cccaf3f6386b66bd9ba4a39e6ec342f89e2c4def361a8cbe7cf/murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)"
"Collecting cymem<2.1.0,>=2.0.2 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/e9/13/3bed1a1d1cce7937eb797d760c0cca973dbdc1891ad7e2f066ae418fd697/cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)"
"Collecting preshed<3.1.0,>=3.0.2 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/42/59/8f65ad22c13020ff281529e415c32a56cfa691d24b0eca2eb3d756e4d644/preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)"
"Collecting thinc<8.2.0,>=8.1.8 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for thinc<8.2.0,>=8.1.8 from https://files.pythonhosted.org/packages/d7/fc/2ea1a37a60ad1c7b9f41699ccd29170f6d479d3349e6742503278b4bc811/thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)"
"Collecting wasabi<1.2.0,>=0.9.1 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata
  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)"
"Collecting srsly<3.0.0,>=2.4.3 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/49/bf/4ea90444abfd1cf12f71ec74b8f95bea5d983145e58001bc022d40151ff4/srsly-2.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading srsly-2.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)"
"Collecting catalogue<2.1.0,>=2.0.6 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/45/8f/5b73efc14e0373d9bb0de6ce1ab04a8f77420dc473f1f3ef270caf085cff/catalogue-2.0.9-py3-none-any.whl.metadata
  Downloading catalogue-2.0.9-py3-none-any.whl.metadata (14 kB)"
"Collecting typer<0.8.0,>=0.3.0 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Downloading typer-0.7.0-py3-none-any.whl (38 kB)"
"Collecting pathy>=0.10.0 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata
  Downloading pathy-0.10.2-py3-none-any.whl.metadata (16 kB)"
"Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (2.31.0)"
"Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 from https://files.pythonhosted.org/packages/bc/e0/0371e9b6c910afe502e5fe18cc94562bfd9399617c7b4f5b6e13c29115b3/pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.3/149.3 kB 7.1 MB/s eta 0:00:00"
Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (3.1.2)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (65.6.3)
"Collecting langcodes<4.0.0,>=3.2.0 (from spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.6/181.6 kB 8.3 MB/s eta 0:00:00"
Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22)) (3.12.3)
Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22)) (1.12)
Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22)) (3.1)
"Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 32.2 MB/s eta 0:00:00"
"Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 13.7 MB/s eta 0:00:00"
"Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 49.4 MB/s eta 0:00:00"
"Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)"
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 1.4 MB/s eta 0:00:00
"Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)"
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 2.1 MB/s eta 0:00:00
"Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 4.3 MB/s eta 0:00:00"
"Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 12.4 MB/s eta 0:00:00"
"Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 6.3 MB/s eta 0:00:00"
"Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 4.1 MB/s eta 0:00:00"
"Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 3.3 MB/s eta 0:00:00"
"Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 3.1 MB/s eta 0:00:00"
"Collecting triton==2.0.0 (from torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.3/63.3 MB 9.8 MB/s eta 0:00:00"
"Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.27.2->-r /opt/ml/model/code/requirements.txt (line 27))
  Obtaining dependency information for huggingface-hub<1.0,>=0.11.0 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata
  Downloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)"
Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22)) (0.38.4)
"Collecting cmake (from triton==2.0.0->torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/de/94/cba4b3ddc0d4555967cce8fd6e9fbced98a6bf62857db71c2400a7b6e183/cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata
  Downloading cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)"
"Collecting lit (from triton==2.0.0->torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22))
  Downloading lit-16.0.6.tar.gz (153 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153.7/153.7 kB 6.9 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Installing backend dependencies: started
  Installing backend dependencies: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'"
"Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Obtaining dependency information for aiohttp!=4.0.0a0,!=4.0.0a1 from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)"
"Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (3.2.0)"
"Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (3.4)"
"Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (1.26.14)"
"Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (2023.7.22)"
"Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/0a/db/223c5b780d1d8eb3ba5263f8c2c6a42502a17702917731f9ca68c4358bfd/blis-0.7.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading blis-0.7.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)"
"Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/93/f8/e89268a1f885048fb2ee6b5c9f93c4e90de768534acfef3652f87d97d4cb/confection-0.1.3-py3-none-any.whl.metadata
  Downloading confection-0.1.3-py3-none-any.whl.metadata (19 kB)"
"Collecting click<9.0.0,>=7.1.1 (from typer<0.8.0,>=0.3.0->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14))
  Obtaining dependency information for click<9.0.0,>=7.1.1 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata
  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)"
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy==3.5.1->-r /opt/ml/model/code/requirements.txt (line 14)) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r /opt/ml/model/code/requirements.txt (line 22)) (1.3.0)
"Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 807.0 kB/s eta 0:00:00"
"Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 4.4 MB/s eta 0:00:00"
"Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata
  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)"
"Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 3.8 MB/s eta 0:00:00"
"Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)"
"Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.4->-r /opt/ml/model/code/requirements.txt (line 10))
  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)"
Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)
"Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.1/46.1 kB 1.6 MB/s eta 0:00:00"
"Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 5.2 MB/s eta 0:00:00"
Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)
"Downloading pathy-0.10.2-py3-none-any.whl (48 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 1.7 MB/s eta 0:00:00"
"Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.9/156.9 kB 5.6 MB/s eta 0:00:00"
"Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 36.2 MB/s eta 0:00:00"
"Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 14.6 MB/s eta 0:00:00"
"Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 2.0 MB/s eta 0:00:00"
"Downloading srsly-2.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 492.9/492.9 kB 7.2 MB/s eta 0:00:00"
"Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (919 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 919.6/919.6 kB 17.4 MB/s eta 0:00:00"
Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)
"Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 10.9 MB/s eta 0:00:00"
"Downloading blis-0.7.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 45.6 MB/s eta 0:00:00"
"Downloading click-8.1.7-py3-none-any.whl (97 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 4.1 MB/s eta 0:00:00"
Downloading confection-0.1.3-py3-none-any.whl (34 kB)
"Downloading cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.1/26.1 MB 21.8 MB/s eta 0:00:00"
"Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 3.1 MB/s eta 0:00:00"
Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)
"Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 9.1 MB/s eta 0:00:00"
"Building wheels for collected packages: sqlitedict, lit
  Building wheel for sqlitedict (setup.py): started
  Building wheel for sqlitedict (setup.py): finished with status 'done'
  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=6685223ce2dd8af03c1762438c4287abda32a74a43de978be603b63539826846
  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd
  Building wheel for lit (pyproject.toml): started
  Building wheel for lit (pyproject.toml): finished with status 'done'
  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=1387568766c1fe62e99920ba43e3660e33b6f1f22e380fa02377562c2ad477d5
  Stored in directory: /root/.cache/pip/wheels/14/f9/07/bb2308587bc2f57158f905a2325f6a89a2befa7437b2d7e137"
Successfully built sqlitedict lit
"Installing collected packages: tokenizers, tensorflow-estimator, sqlitedict, lit, keras, cymem, cmake, wasabi, tqdm, tensorflow-io-gcs-filesystem, spacy-loggers, spacy-legacy, smart-open, scipy, regex, pytorch-crf, pydantic, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, murmurhash, multidict, lightning-utilities, langcodes, Keras-Preprocessing, joblib, fsspec, frozenlist, click, catalogue, blis, attrs, async-timeout, yarl, typer, srsly, SoMaJo, scikit-learn, preshed, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, matplotlib, huggingface-hub, gensim, aiosignal, transformers, pathy, confection, aiohttp, thinc, spacy, de-core-news-sm, triton, torch, torchmetrics, torchvision, torchaudio, pytorch-lightning
  Attempting uninstall: tqdm
    Found existing installation: tqdm 4.64.1
    Uninstalling tqdm-4.64.1:
      Successfully uninstalled tqdm-4.64.1
  Attempting uninstall: scipy
    Found existing installation: scipy 1.11.2
    Uninstalling scipy-1.11.2:"
      Successfully uninstalled scipy-1.11.2
"  Attempting uninstall: joblib
    Found existing installation: joblib 1.3.2
    Uninstalling joblib-1.3.2:
      Successfully uninstalled joblib-1.3.2
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 1.3.0
    Uninstalling scikit-learn-1.3.0:
      Successfully uninstalled scikit-learn-1.3.0
  Attempting uninstall: pandas
    Found existing installation: pandas 2.1.0
    Uninstalling pandas-2.1.0:
      Successfully uninstalled pandas-2.1.0"
"  Attempting uninstall: matplotlib
    Found existing installation: matplotlib 3.7.2
    Uninstalling matplotlib-3.7.2:
      Successfully uninstalled matplotlib-3.7.2"
"  Attempting uninstall: torch
    Found existing installation: torch 2.0.1+cpu
    Uninstalling torch-2.0.1+cpu:
      Successfully uninstalled torch-2.0.1+cpu"
"  Attempting uninstall: torchvision
    Found existing installation: torchvision 0.15.2+cpu
    Uninstalling torchvision-0.15.2+cpu:
      Successfully uninstalled torchvision-0.15.2+cpu
  Attempting uninstall: torchaudio
    Found existing installation: torchaudio 2.0.2+cpu
    Uninstalling torchaudio-2.0.2+cpu:
      Successfully uninstalled torchaudio-2.0.2+cpu"
Successfully installed Keras-Preprocessing-1.1.2 SoMaJo-2.2.3 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 blis-0.7.10 catalogue-2.0.9 click-8.1.7 cmake-3.27.5 confection-0.1.3 cymem-2.0.8 de-core-news-sm-3.5.0 frozenlist-1.4.0 fsspec-2023.9.1 gensim-4.3.1 huggingface-hub-0.17.2 joblib-1.2.0 keras-2.7.0 langcodes-3.3.0 lightning-utilities-0.8.0 lit-16.0.6 matplotlib-3.7.1 multidict-6.0.4 murmurhash-1.0.10 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-1.3.5 pathy-0.10.2 preshed-3.0.9 pydantic-1.10.12 pytorch-crf-0.7.2 pytorch-lightning-1.9.4 regex-2023.8.8 scikit-learn-1.2.2 scipy-1.10.1 smart-open-6.4.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 sqlitedict-2.1.0 srsly-2.4.7 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.31.0 thinc-8.1.12 tokenizers-0.12.1 torch-2.0.0 torchaudio-2.0.1 torchmetrics-0.11.4 torchvision-0.15.1 tqdm-4.62.3 transformers-4.27.2 triton-2.0.0 typer-0.7.0 wasabi-1.1.2 yarl-1.9.2
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport
WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
"2023-09-19T08:47:46,640 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties"
"2023-09-19T08:47:46,643 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager..."
"2023-09-19T08:47:46,700 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml"
"2023-09-19T08:47:46,796 [INFO ] main org.pytorch.serve.ModelServer - "
Torchserve version: 0.8.2
TS Home: /opt/conda/lib/python3.10/site-packages
Current directory: /
Temp directory: /home/model-server/tmp
Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 7924 M
Python executable: /opt/conda/bin/python3.10
Config file: /etc/sagemaker-ts.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8080
Metrics address: http://127.0.0.1:8082
Model Store: /.sagemaker/ts/models
Initial Models: model=/opt/ml/model
Log dir: /logs
Metrics dir: /logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /.sagemaker/ts/models
Model config: N/A
"2023-09-19T08:47:46,803 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin..."
"2023-09-19T08:47:46,823 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model"
"2023-09-19T08:47:46,827 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher"
"2023-09-19T08:47:46,827 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher"
"2023-09-19T08:47:46,829 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded."
"2023-09-19T08:47:46,841 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel."
"2023-09-19T08:47:47,064 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080"
"2023-09-19T08:47:47,064 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel."
"2023-09-19T08:47:47,143 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082"
Model server started.
"2023-09-19T08:47:47,766 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet."
"2023-09-19T08:47:47,951 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:47,956 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:32.72339630126953|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:47,957 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:19.26486587524414|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:47,958 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:37.1|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:47,959 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29155.85546875|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:47,959 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2083.125|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:47,960 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:8.0|#Level:Host|#hostname:container-0.local,timestamp:1695113267"
"2023-09-19T08:47:49,318 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=141"
"2023-09-19T08:47:49,320 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:47:49,327 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=135"
"2023-09-19T08:47:49,328 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:47:49,338 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,338 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]141"
"2023-09-19T08:47:49,339 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,344 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,345 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]135"
"2023-09-19T08:47:49,345 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,346 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,346 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:47:49,345 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:47:49,350 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,357 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:47:49,360 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:47:49,361 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269361"
"2023-09-19T08:47:49,372 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269372"
"2023-09-19T08:47:49,380 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=138"
"2023-09-19T08:47:49,381 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:47:49,382 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=140"
"2023-09-19T08:47:49,385 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:47:49,399 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,398 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=137"
"2023-09-19T08:47:49,400 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]138"
"2023-09-19T08:47:49,400 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:47:49,400 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,401 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,400 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:47:49,406 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,407 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]140"
"2023-09-19T08:47:49,408 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,408 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:47:49,410 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,410 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269410"
"2023-09-19T08:47:49,411 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:47:49,412 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,413 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,416 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:47:49,417 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269416"
"2023-09-19T08:47:49,424 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,425 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]137"
"2023-09-19T08:47:49,425 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,426 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:47:49,426 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,428 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:47:49,429 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269429"
"2023-09-19T08:47:49,454 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,455 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,448 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=139"
"2023-09-19T08:47:49,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=142"
"2023-09-19T08:47:49,473 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:47:49,474 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:47:49,474 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,474 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]139"
"2023-09-19T08:47:49,475 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,475 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,476 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,483 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,483 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:47:49,484 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]142"
"2023-09-19T08:47:49,484 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,485 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:47:49,486 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,487 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:47:49,487 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269487"
"2023-09-19T08:47:49,500 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:47:49,500 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269500"
"2023-09-19T08:47:49,527 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,545 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,608 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=136"
"2023-09-19T08:47:49,609 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:47:49,639 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:49,650 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]136"
"2023-09-19T08:47:49,651 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:49,651 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:47:49,652 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:49,654 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1695113269654"
"2023-09-19T08:47:49,666 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:47:49,677 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,678 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,679 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,679 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,679 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,680 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,680 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,681 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,681 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,688 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,692 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,693 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,693 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,693 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,693 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,693 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,693 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,681 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,696 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,694 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,696 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,697 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,697 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,695 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,696 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,698 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,698 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,699 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,700 [INFO ] W-9005-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,700 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,701 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269701"
"2023-09-19T08:47:49,701 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:47:49,702 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:47:49,703 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds."
"2023-09-19T08:47:49,700 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,703 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,705 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,706 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,706 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 179, in validate_and_initialize"
"2023-09-19T08:47:49,706 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()"
"2023-09-19T08:47:49,707 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 206, in _validate_user_module_and_set_functions"
"2023-09-19T08:47:49,707 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)"
"2023-09-19T08:47:49,710 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,710 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/importlib/__init__.py"", line 126, in import_module"
"2023-09-19T08:47:49,711 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)"
"2023-09-19T08:47:49,710 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:47:49,711 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,711 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,712 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,712 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,712 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,713 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,713 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,713 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,713 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,714 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,714 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,714 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,715 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,715 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,716 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,716 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 179, in validate_and_initialize"
"2023-09-19T08:47:49,717 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()"
"2023-09-19T08:47:49,717 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 206, in _validate_user_module_and_set_functions"
"2023-09-19T08:47:49,717 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)"
"2023-09-19T08:47:49,718 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/importlib/__init__.py"", line 126, in import_module"
"2023-09-19T08:47:49,718 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)"
"2023-09-19T08:47:49,719 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import"
"2023-09-19T08:47:49,719 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import"
"2023-09-19T08:47:49,720 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load"
"2023-09-19T08:47:49,723 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked"
"2023-09-19T08:47:49,720 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,724 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,724 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269724"
"2023-09-19T08:47:49,724 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:47:49,725 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:47:49,725 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds."
"2023-09-19T08:47:49,719 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load"
"2023-09-19T08:47:49,726 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:47:49,726 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked"
"2023-09-19T08:47:49,726 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 879, in exec_module"
"2023-09-19T08:47:49,729 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 1017, in get_code"
"2023-09-19T08:47:49,729 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 947, in source_to_code"
"2023-09-19T08:47:49,727 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,730 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,730 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269730"
"2023-09-19T08:47:49,731 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:47:49,731 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:47:49,731 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds."
"2023-09-19T08:47:49,732 [INFO ] W-9006-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed"
"2023-09-19T08:47:49,736 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:47:49,762 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,762 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,762 [INFO ] W-9004-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,762 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,762 [INFO ] W-9004-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,763 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,763 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,763 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269763"
"2023-09-19T08:47:49,763 [INFO ] W-9004-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,763 [INFO ] W-9004-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,763 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:47:49,764 [INFO ] W-9004-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,764 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:47:49,764 [INFO ] W-9004-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,764 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds."
"2023-09-19T08:47:49,764 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:47:49,763 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,764 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,764 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,765 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 179, in validate_and_initialize"
"2023-09-19T08:47:49,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 206, in _validate_user_module_and_set_functions"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/importlib/__init__.py"", line 126, in import_module"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked"
"2023-09-19T08:47:49,767 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 879, in exec_module"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 1017, in get_code"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 947, in source_to_code"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File ""/opt/ml/model/code/inference.py"", line 15"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     def create_connection(host, user, pw, db_name, port)"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -                                                         ^"
"2023-09-19T08:47:49,768 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - SyntaxError: expected ':'"
"2023-09-19T08:47:49,775 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:47:49,769 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,776 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,776 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,776 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,776 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269776"
"2023-09-19T08:47:49,776 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,777 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,777 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,777 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:47:49,777 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:47:49,777 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,777 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds."
"2023-09-19T08:47:49,777 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,777 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,777 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,778 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,778 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,778 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,778 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 179, in validate_and_initialize"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 206, in _validate_user_module_and_set_functions"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/importlib/__init__.py"", line 126, in import_module"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import"
"2023-09-19T08:47:49,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 879, in exec_module"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 1017, in get_code"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 947, in source_to_code"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File ""/opt/ml/model/code/inference.py"", line 15"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     def create_connection(host, user, pw, db_name, port)"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -                                                         ^"
"2023-09-19T08:47:49,780 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: expected ':'"
"2023-09-19T08:47:49,781 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,783 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,783 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269783"
"2023-09-19T08:47:49,784 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:47:49,786 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1"
"2023-09-19T08:47:49,784 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:47:49,787 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds."
"2023-09-19T08:47:49,788 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,789 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 179, in validate_and_initialize"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 206, in _validate_user_module_and_set_functions"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/importlib/__init__.py"", line 126, in import_module"
"2023-09-19T08:47:49,790 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 879, in exec_module"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 1017, in get_code"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 947, in source_to_code"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed"
"2023-09-19T08:47:49,791 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -   File ""/opt/ml/model/code/inference.py"", line 15"
"2023-09-19T08:47:49,792 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -     def create_connection(host, user, pw, db_name, port)"
"2023-09-19T08:47:49,792 [INFO ] W-9007-model_1.0-stdout MODEL_LOG -                                                         ^"
"2023-09-19T08:47:49,792 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - SyntaxError: expected ':'"
"2023-09-19T08:47:49,797 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,798 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,798 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269798"
"2023-09-19T08:47:49,798 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:47:49,798 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:47:49,799 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds."
"2023-09-19T08:47:49,803 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:47:49,804 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:47:49,804 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:47:49,804 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:47:49,803 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:47:49,806 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:47:49,817 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:47:49,817 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:47:49,907 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died."
"2023-09-19T08:47:49,907 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):"
"2023-09-19T08:47:49,907 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 253, in <module>"
"2023-09-19T08:47:49,907 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()"
"2023-09-19T08:47:49,907 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 221, in run_server"
"2023-09-19T08:47:49,907 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 184, in handle_connection"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py"", line 131, in load_model"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load("
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/ts/model_loader.py"", line 135, in load"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py"", line 51, in initialize"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py"", line 66, in initialize"
"2023-09-19T08:47:49,908 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 179, in validate_and_initialize"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py"", line 206, in _validate_user_module_and_set_functions"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.10/importlib/__init__.py"", line 126, in import_module"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 879, in exec_module"
"2023-09-19T08:47:49,909 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 1017, in get_code"
"2023-09-19T08:47:49,910 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap_external>"", line 947, in source_to_code"
"2023-09-19T08:47:49,910 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed"
"2023-09-19T08:47:49,910 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File ""/opt/ml/model/code/inference.py"", line 15"
"2023-09-19T08:47:49,910 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     def create_connection(host, user, pw, db_name, port)"
"2023-09-19T08:47:49,910 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -                                                         ^"
"2023-09-19T08:47:49,910 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - SyntaxError: expected ':'"
"2023-09-19T08:47:49,911 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:49,912 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died."
"2023-09-19T08:47:49,912 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1695113269912"
"2023-09-19T08:47:49,912 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:47:49,912 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:47:49,912 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds."
"2023-09-19T08:47:49,923 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:47:49,923 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:47:50,549 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 11"
"2023-09-19T08:47:50,549 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113270"
"2023-09-19T08:47:53,051 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=314"
"2023-09-19T08:47:53,052 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:47:53,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,072 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]314"
"2023-09-19T08:47:53,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,075 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:47:53,076 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,078 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:47:53,078 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,079 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,079 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:47:53,079 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:47:53,080 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds."
"2023-09-19T08:47:53,107 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:47:53,107 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:47:53,114 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=299"
"2023-09-19T08:47:53,117 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:47:53,122 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=305"
"2023-09-19T08:47:53,123 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:47:53,123 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=302"
"2023-09-19T08:47:53,123 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:47:53,133 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,134 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]299"
"2023-09-19T08:47:53,134 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,135 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:47:53,135 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,136 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:47:53,136 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,136 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,137 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:47:53,137 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:47:53,137 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds."
"2023-09-19T08:47:53,139 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,140 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]305"
"2023-09-19T08:47:53,140 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,140 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:47:53,141 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,142 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:47:53,142 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=311"
"2023-09-19T08:47:53,142 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,143 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,143 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:47:53,143 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:47:53,143 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:47:53,143 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds."
"2023-09-19T08:47:53,144 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,144 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]302"
"2023-09-19T08:47:53,145 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,145 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:47:53,145 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,147 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:47:53,147 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,148 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,148 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:47:53,148 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:47:53,148 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds."
"2023-09-19T08:47:53,152 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:47:53,152 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:47:53,160 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:47:53,160 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,161 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:47:53,161 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]311"
"2023-09-19T08:47:53,161 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,161 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:47:53,162 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,163 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,163 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:47:53,164 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,164 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:47:53,164 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:47:53,164 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:47:53,165 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds."
"2023-09-19T08:47:53,166 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:47:53,170 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=317"
"2023-09-19T08:47:53,171 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:47:53,179 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:47:53,180 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:47:53,184 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,185 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]317"
"2023-09-19T08:47:53,185 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,185 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,185 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:47:53,186 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,186 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:47:53,186 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,187 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:47:53,187 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:47:53,187 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds."
"2023-09-19T08:47:53,198 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:47:53,198 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:47:53,199 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=308"
"2023-09-19T08:47:53,199 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:47:53,213 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,213 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]308"
"2023-09-19T08:47:53,213 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,214 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,214 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:47:53,215 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,215 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:47:53,215 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,215 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:47:53,215 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:47:53,215 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds."
"2023-09-19T08:47:53,219 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=320"
"2023-09-19T08:47:53,219 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:47:53,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:53,231 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:47:53,231 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:47:53,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]320"
"2023-09-19T08:47:53,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:53,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:53,231 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:47:53,233 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:53,233 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:47:53,233 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:53,233 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:47:53,233 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:47:53,233 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds."
"2023-09-19T08:47:53,243 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:47:53,243 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:47:55,512 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 15"
"2023-09-19T08:47:55,524 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113275"
"2023-09-19T08:47:56,299 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=419"
"2023-09-19T08:47:56,302 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:47:56,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,321 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]419"
"2023-09-19T08:47:56,322 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,323 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:47:56,324 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,325 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,326 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:47:56,326 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:47:56,327 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds."
"2023-09-19T08:47:56,358 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:47:56,358 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:47:56,343 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:47:56,491 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=431"
"2023-09-19T08:47:56,493 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:47:56,494 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=425"
"2023-09-19T08:47:56,497 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:47:56,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=428"
"2023-09-19T08:47:56,501 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:47:56,508 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,508 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]431"
"2023-09-19T08:47:56,509 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,509 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:56,509 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:47:56,510 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,511 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,511 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:47:56,511 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:47:56,511 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds."
"2023-09-19T08:47:56,512 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,513 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]425"
"2023-09-19T08:47:56,513 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,514 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:47:56,514 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,515 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,515 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:47:56,515 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:47:56,515 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds."
"2023-09-19T08:47:56,517 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,517 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]428"
"2023-09-19T08:47:56,518 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,518 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:47:56,518 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:56,519 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:47:56,519 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,519 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,519 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:47:56,520 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:47:56,520 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds."
"2023-09-19T08:47:56,534 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:47:56,535 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:47:56,536 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:47:56,536 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:47:56,536 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:47:56,540 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=437"
"2023-09-19T08:47:56,541 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:47:56,543 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:47:56,543 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:47:56,543 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:47:56,552 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=434"
"2023-09-19T08:47:56,553 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:47:56,557 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,558 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]437"
"2023-09-19T08:47:56,558 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:47:56,558 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,558 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:56,559 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:47:56,559 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,560 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,560 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:47:56,560 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:47:56,560 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds."
"2023-09-19T08:47:56,564 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=440"
"2023-09-19T08:47:56,565 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:47:56,565 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,565 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]434"
"2023-09-19T08:47:56,566 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,566 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:47:56,566 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:56,566 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,567 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:47:56,567 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,567 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:47:56,567 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:47:56,568 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds."
"2023-09-19T08:47:56,579 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,579 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]440"
"2023-09-19T08:47:56,579 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,580 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:56,580 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:47:56,580 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,581 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,581 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:47:56,581 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:47:56,584 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:47:56,585 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:47:56,584 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:47:56,585 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds."
"2023-09-19T08:47:56,581 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:47:56,585 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:47:56,585 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:47:56,585 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=422"
"2023-09-19T08:47:56,586 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:47:56,592 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:47:56,596 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:47:56,596 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]422"
"2023-09-19T08:47:56,596 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:47:56,596 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:47:56,596 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:47:56,597 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:47:56,597 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:47:56,597 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:47:56,598 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:47:56,598 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:47:56,598 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds."
"2023-09-19T08:47:56,608 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:47:56,608 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:48:00,495 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:48:00,496 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113280"
"2023-09-19T08:48:00,525 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=503"
"2023-09-19T08:48:00,539 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:00,557 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,580 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]503"
"2023-09-19T08:48:00,581 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,581 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:00,583 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,584 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,584 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,585 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:48:00,585 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:48:00,585 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds."
"2023-09-19T08:48:00,612 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:48:00,612 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:48:00,612 [ERROR] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:48:00,752 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=509"
"2023-09-19T08:48:00,753 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:00,768 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,769 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]509"
"2023-09-19T08:48:00,770 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,770 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,770 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:00,771 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,772 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,773 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:48:00,773 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:48:00,773 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds."
"2023-09-19T08:48:00,787 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:48:00,788 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:48:00,860 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=518"
"2023-09-19T08:48:00,861 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:00,870 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,870 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]518"
"2023-09-19T08:48:00,871 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,871 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,871 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:00,872 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:48:00,872 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,872 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,873 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:48:00,873 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:48:00,873 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds."
"2023-09-19T08:48:00,887 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:48:00,887 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:48:00,888 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=506"
"2023-09-19T08:48:00,888 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:00,900 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=515"
"2023-09-19T08:48:00,900 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:00,905 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,905 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]506"
"2023-09-19T08:48:00,905 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,906 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:00,906 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,907 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,907 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:48:00,907 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,907 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:48:00,908 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:48:00,908 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds."
"2023-09-19T08:48:00,916 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,917 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]515"
"2023-09-19T08:48:00,917 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,917 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,917 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:00,918 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,918 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:48:00,918 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,918 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:48:00,919 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:48:00,919 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds."
"2023-09-19T08:48:00,919 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:48:00,919 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:48:00,929 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:48:00,929 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:48:00,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=512"
"2023-09-19T08:48:00,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:00,939 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=524"
"2023-09-19T08:48:00,939 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,939 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]512"
"2023-09-19T08:48:00,939 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:00,939 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,940 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:00,940 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,940 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:48:00,940 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,940 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,941 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:48:00,941 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:48:00,941 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds."
"2023-09-19T08:48:00,953 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,953 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]524"
"2023-09-19T08:48:00,953 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,953 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,953 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:00,954 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,954 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:48:00,954 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,954 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:48:00,954 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:48:00,954 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds."
"2023-09-19T08:48:00,956 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:48:00,956 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:48:00,964 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:48:00,964 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:48:00,971 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=521"
"2023-09-19T08:48:00,971 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:00,980 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:00,980 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]521"
"2023-09-19T08:48:00,980 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:00,980 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:00,981 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:00,981 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:00,981 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:48:00,981 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:00,981 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:48:00,982 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:48:00,982 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds."
"2023-09-19T08:48:00,992 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:48:00,992 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:48:05,497 [INFO ] pool-2-thread-9 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:05,497 [INFO ] pool-2-thread-9 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113285"
"2023-09-19T08:48:05,666 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=583"
"2023-09-19T08:48:05,668 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:05,686 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:05,687 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]583"
"2023-09-19T08:48:05,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:05,688 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:05,690 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:05,691 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:05,692 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:48:05,693 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:48:05,693 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds."
"2023-09-19T08:48:05,711 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:48:05,712 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:48:05,712 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:48:05,945 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=586"
"2023-09-19T08:48:05,946 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:05,963 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:05,963 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]586"
"2023-09-19T08:48:05,963 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:05,963 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:05,964 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:05,965 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:05,965 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:05,966 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:48:05,966 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:48:05,966 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds."
"2023-09-19T08:48:05,965 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:48:05,967 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:48:05,987 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:48:06,084 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=601"
"2023-09-19T08:48:06,085 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:06,094 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:06,094 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]601"
"2023-09-19T08:48:06,094 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:06,094 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:06,094 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:06,095 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:06,095 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:06,096 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:48:06,096 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:48:06,096 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds."
"2023-09-19T08:48:06,096 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:48:06,096 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:48:06,106 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:48:06,179 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=598"
"2023-09-19T08:48:06,179 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:06,186 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=595"
"2023-09-19T08:48:06,186 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:06,196 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:06,196 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]595"
"2023-09-19T08:48:06,196 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:06,196 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:06,196 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:06,196 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:06,196 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]598"
"2023-09-19T08:48:06,197 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:06,197 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:06,197 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:06,197 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:48:06,197 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:48:06,197 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:06,198 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:06,198 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:06,198 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:06,198 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:48:06,198 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:48:06,198 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds."
"2023-09-19T08:48:06,198 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:48:06,198 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:48:06,198 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds."
"2023-09-19T08:48:06,210 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:48:06,210 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:48:06,216 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:48:06,216 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:48:06,269 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=592"
"2023-09-19T08:48:06,269 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:06,278 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:06,278 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]592"
"2023-09-19T08:48:06,278 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:06,278 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:06,279 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:06,279 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:06,279 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:48:06,280 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:06,280 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:48:06,280 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:48:06,280 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds."
"2023-09-19T08:48:06,290 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:48:06,290 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:48:06,311 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=589"
"2023-09-19T08:48:06,311 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:06,321 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:06,321 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]589"
"2023-09-19T08:48:06,321 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:06,321 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:06,321 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:06,322 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:06,322 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:48:06,322 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:06,322 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:48:06,322 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:48:06,322 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds."
"2023-09-19T08:48:06,332 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=604"
"2023-09-19T08:48:06,332 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:06,332 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:48:06,333 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:48:06,342 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:06,342 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]604"
"2023-09-19T08:48:06,342 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:06,342 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:06,342 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:06,343 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:06,343 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:48:06,343 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:06,343 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:48:06,343 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:48:06,343 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds."
"2023-09-19T08:48:06,353 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:48:06,353 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:48:10,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:48:10,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113290"
"2023-09-19T08:48:12,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=663"
"2023-09-19T08:48:12,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:12,708 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:12,709 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]663"
"2023-09-19T08:48:12,710 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:12,710 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:12,713 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:12,714 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:12,714 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:48:12,715 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:48:12,715 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds."
"2023-09-19T08:48:12,731 [ERROR] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:48:12,733 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:48:12,734 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:48:13,182 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=666"
"2023-09-19T08:48:13,182 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:13,198 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,198 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]666"
"2023-09-19T08:48:13,199 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:13,199 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,200 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,200 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,202 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:48:13,202 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:48:13,202 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds."
"2023-09-19T08:48:13,202 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,203 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:48:13,216 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:48:13,364 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=673"
"2023-09-19T08:48:13,365 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:13,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]673"
"2023-09-19T08:48:13,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,375 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,375 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:13,375 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,375 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:48:13,375 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,376 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:48:13,376 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:48:13,376 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds."
"2023-09-19T08:48:13,390 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:48:13,391 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:48:13,439 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=669"
"2023-09-19T08:48:13,439 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:13,456 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,456 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]669"
"2023-09-19T08:48:13,456 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,456 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,456 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:13,457 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,457 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:48:13,457 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,458 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:48:13,462 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:48:13,462 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds."
"2023-09-19T08:48:13,474 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:48:13,475 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:48:13,541 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=672"
"2023-09-19T08:48:13,542 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:13,551 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,551 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]672"
"2023-09-19T08:48:13,551 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,551 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,551 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:13,552 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,552 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:48:13,552 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,552 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:48:13,552 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:48:13,552 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds."
"2023-09-19T08:48:13,559 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=688"
"2023-09-19T08:48:13,560 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:13,567 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:48:13,567 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:48:13,569 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,569 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]688"
"2023-09-19T08:48:13,569 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,569 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,569 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:13,570 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,570 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:48:13,570 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,570 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:48:13,570 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:48:13,571 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds."
"2023-09-19T08:48:13,585 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:48:13,585 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:48:13,606 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=685"
"2023-09-19T08:48:13,607 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:13,616 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,616 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]685"
"2023-09-19T08:48:13,616 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,616 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,616 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:13,617 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,617 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:48:13,617 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,617 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:48:13,617 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:48:13,617 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds."
"2023-09-19T08:48:13,627 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:48:13,627 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:48:13,660 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=690"
"2023-09-19T08:48:13,660 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:13,669 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:13,669 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]690"
"2023-09-19T08:48:13,669 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:13,669 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:13,670 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:13,670 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:48:13,670 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:13,670 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:13,671 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:48:13,671 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:48:13,671 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds."
"2023-09-19T08:48:13,681 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:48:13,681 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:48:15,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:15,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113295"
"2023-09-19T08:48:20,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:20,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113300"
"2023-09-19T08:48:22,532 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=743"
"2023-09-19T08:48:22,533 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:22,551 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:22,552 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]743"
"2023-09-19T08:48:22,552 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:22,552 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:22,552 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:22,554 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:22,555 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:22,555 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:48:22,555 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:48:22,556 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds."
"2023-09-19T08:48:22,597 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:48:22,608 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:48:22,609 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:48:23,400 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=746"
"2023-09-19T08:48:23,401 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:23,417 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,417 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]746"
"2023-09-19T08:48:23,418 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:23,417 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,418 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,420 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,420 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:48:23,421 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,421 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:48:23,421 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:48:23,421 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds."
"2023-09-19T08:48:23,437 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:48:23,437 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:48:23,506 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=765"
"2023-09-19T08:48:23,507 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:23,516 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,516 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]765"
"2023-09-19T08:48:23,517 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,517 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:23,517 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,517 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,518 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,518 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:48:23,518 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:48:23,518 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds."
"2023-09-19T08:48:23,517 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:48:23,522 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:48:23,528 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:48:23,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=756"
"2023-09-19T08:48:23,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:23,554 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,555 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]756"
"2023-09-19T08:48:23,555 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,555 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,555 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:23,556 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,556 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:48:23,556 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,556 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:48:23,556 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:48:23,557 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds."
"2023-09-19T08:48:23,567 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:48:23,567 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:48:23,769 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=762"
"2023-09-19T08:48:23,769 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:23,779 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,779 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]762"
"2023-09-19T08:48:23,779 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,779 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,779 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:23,780 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:48:23,780 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,780 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,780 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:48:23,780 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:48:23,780 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds."
"2023-09-19T08:48:23,795 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:48:23,795 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:48:23,811 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=771"
"2023-09-19T08:48:23,811 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:23,820 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,820 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]771"
"2023-09-19T08:48:23,820 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,820 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,821 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:23,821 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,821 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:48:23,821 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,821 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:48:23,822 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:48:23,822 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds."
"2023-09-19T08:48:23,832 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:48:23,832 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:48:23,835 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=759"
"2023-09-19T08:48:23,835 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:23,846 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,846 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]759"
"2023-09-19T08:48:23,846 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,846 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,846 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:23,847 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,847 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:48:23,847 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,847 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:48:23,847 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:48:23,847 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds."
"2023-09-19T08:48:23,857 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:48:23,857 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:48:23,886 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=768"
"2023-09-19T08:48:23,887 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:23,896 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:23,896 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]768"
"2023-09-19T08:48:23,896 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:23,896 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:23,896 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:23,897 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:23,897 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:48:23,897 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:23,897 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:48:23,897 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:48:23,897 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds."
"2023-09-19T08:48:23,907 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:48:23,907 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:48:25,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:25,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113305"
"2023-09-19T08:48:30,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:30,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113310"
"2023-09-19T08:48:35,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:48:35,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113315"
"2023-09-19T08:48:37,007 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=823"
"2023-09-19T08:48:37,008 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:37,025 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:37,043 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]823"
"2023-09-19T08:48:37,044 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:37,044 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:37,045 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:37,046 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:37,047 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:48:37,047 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:48:37,070 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds."
"2023-09-19T08:48:37,078 [ERROR] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:48:37,094 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:48:37,096 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:48:38,397 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=833"
"2023-09-19T08:48:38,398 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:38,414 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:38,414 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]833"
"2023-09-19T08:48:38,415 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:48:38,416 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:38,415 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:38,416 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:38,416 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:48:38,416 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:38,417 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:48:38,417 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:48:38,417 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds."
"2023-09-19T08:48:38,432 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:48:38,432 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:48:38,583 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=836"
"2023-09-19T08:48:38,583 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:38,599 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:38,600 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]836"
"2023-09-19T08:48:38,600 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:38,600 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:38,600 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:48:38,601 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:38,601 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:48:38,601 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:38,601 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:48:38,601 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:48:38,601 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds."
"2023-09-19T08:48:38,616 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:48:38,616 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:48:38,723 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=839"
"2023-09-19T08:48:38,723 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:38,732 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:38,733 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]839"
"2023-09-19T08:48:38,733 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:38,733 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:38,733 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:48:38,734 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:38,734 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:48:38,734 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:38,734 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:48:38,734 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:48:38,734 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds."
"2023-09-19T08:48:38,749 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:48:38,749 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:48:38,807 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=845"
"2023-09-19T08:48:38,808 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:38,823 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:38,823 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]845"
"2023-09-19T08:48:38,823 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:38,823 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:38,823 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:48:38,824 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:38,824 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:48:38,824 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:38,824 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:48:38,824 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:48:38,825 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds."
"2023-09-19T08:48:38,835 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:48:38,835 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:48:38,981 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=851"
"2023-09-19T08:48:38,982 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:38,991 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:38,991 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]851"
"2023-09-19T08:48:38,991 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:38,991 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:38,991 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:48:38,992 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:38,992 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:48:38,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:38,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:48:38,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:48:38,992 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds."
"2023-09-19T08:48:39,007 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:48:39,007 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:48:39,055 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=848"
"2023-09-19T08:48:39,055 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:39,064 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:39,065 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]848"
"2023-09-19T08:48:39,065 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:39,065 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:39,065 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:48:39,065 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:39,065 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:48:39,065 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:39,066 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:48:39,066 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:48:39,066 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds."
"2023-09-19T08:48:39,069 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=842"
"2023-09-19T08:48:39,069 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:39,077 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:48:39,076 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:48:39,079 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:39,079 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]842"
"2023-09-19T08:48:39,079 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:39,079 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:39,079 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:48:39,080 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:39,080 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:48:39,080 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:39,080 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:48:39,080 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:48:39,080 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds."
"2023-09-19T08:48:39,090 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:48:39,090 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:48:40,495 [INFO ] W-9004-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:40,495 [INFO ] W-9004-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113320"
"2023-09-19T08:48:45,495 [INFO ] W-9004-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:45,495 [INFO ] W-9004-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113325"
"2023-09-19T08:48:47,771 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,772 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:33.09054946899414|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,772 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.89771270751953|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,772 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:36.3|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,772 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29324.00390625|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,772 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1915.01953125|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,772 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:7.5|#Level:Host|#hostname:container-0.local,timestamp:1695113327"
"2023-09-19T08:48:47,786 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/833/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=833)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=833)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('833',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('833', 0)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/851/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=851)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=851)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('851',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('851', 0)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/836/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=836)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=836)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('836',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('836', 0)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/839/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=839)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=839)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('839',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('839', 0)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/823/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=823)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=823)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('823',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('823', 0)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/842/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=842)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=842)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('842',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('842', 0)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 480, in wrapper
    raise raise_from(err, None)
  File ""<string>"", line 3, in raise_from
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 478, in wrapper
    return fun(self)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1695, in _parse_stat_file
    data = bcat(""%s/%s/stat"" % (self._procfs_path, self.pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 813, in bcat
    return cat(fname, fallback=fallback, _open=open_binary)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 801, in cat
    with _open(fname) as f:
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_common.py"", line 765, in open_binary
    return open(fname, ""rb"", buffering=FILE_READ_BUFFER_SIZE)"
FileNotFoundError: [Errno 2] No such file or directory: '/proc/845/stat'
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 361, in _init
    self.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 719, in create_time
    self._create_time = self._proc.create_time()
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1863, in create_time
    ctime = float(self._parse_stat_file()['create_time'])
  File ""/opt/conda/lib/python3.10/site-packages/psutil/_pslinux.py"", line 1660, in wrapper
    raise NoSuchProcess(self.pid, self._name)"
psutil.NoSuchProcess: process no longer exists (pid=845)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 332, in __init__
    self._init(pid)
  File ""/opt/conda/lib/python3.10/site-packages/psutil/__init__.py"", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')"
psutil.NoSuchProcess: process PID not found (pid=845)
"During handling of the above exception, another exception occurred:"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 22, in get_cpu_usage
    logging.error(""Failed get process for pid: %s"", pid, exc_info=True)"
Message: 'Failed get process for pid: %s'
"Arguments: ('845',)"
'"--- Logging error ---"
"Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1104, in emit
    self.flush()
  File ""/opt/conda/lib/python3.10/logging/__init__.py"", line 1084, in flush
    self.stream.flush()"
BrokenPipeError: [Errno 32] Broken pipe
"Call stack:
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/metric_collector.py"", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File ""/opt/conda/lib/python3.10/site-packages/ts/metrics/process_memory_metric.py"", line 40, in check_process_mem_usage
    logging.info(""%s:%d"", process, get_cpu_usage(process))"
Message: '%s:%d'
"Arguments: ('845', 0)"
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
"2023-09-19T08:48:50,494 [INFO ] W-9004-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:48:50,495 [INFO ] W-9004-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113330"
"2023-09-19T08:48:55,495 [INFO ] W-9004-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:48:55,495 [INFO ] W-9004-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113335"
"2023-09-19T08:48:59,405 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=905"
"2023-09-19T08:48:59,406 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:59,415 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:48:59,415 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]905"
"2023-09-19T08:48:59,415 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:48:59,415 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:48:59,415 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:48:59,416 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:48:59,416 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:48:59,416 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:48:59,416 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:48:59,416 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:48:59,416 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds."
"2023-09-19T08:48:59,426 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:48:59,426 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:49:00,499 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 4"
"2023-09-19T08:49:00,500 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113340"
"2023-09-19T08:49:01,469 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=915"
"2023-09-19T08:49:01,471 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:49:01,486 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:01,487 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]915"
"2023-09-19T08:49:01,487 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:01,487 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:01,487 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:49:01,488 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:01,488 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:49:01,488 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:01,489 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:49:01,489 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:49:01,489 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 34 seconds."
"2023-09-19T08:49:01,504 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:49:01,505 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:49:01,746 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=918"
"2023-09-19T08:49:01,746 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:49:01,764 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:01,764 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]918"
"2023-09-19T08:49:01,764 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:01,764 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:01,764 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:49:01,765 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:01,765 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:01,766 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:49:01,766 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:49:01,771 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 34 seconds."
"2023-09-19T08:49:01,771 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:49:01,781 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:49:01,811 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=921"
"2023-09-19T08:49:01,812 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:49:01,821 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:01,821 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]921"
"2023-09-19T08:49:01,822 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:01,822 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:01,822 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:49:01,822 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:01,822 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:49:01,823 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:01,823 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:49:01,823 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:49:01,823 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds."
"2023-09-19T08:49:01,833 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:49:01,833 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:49:01,880 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=938"
"2023-09-19T08:49:01,880 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:49:01,889 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:01,890 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]938"
"2023-09-19T08:49:01,890 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:01,890 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:01,890 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:49:01,890 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:01,890 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:49:01,891 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:01,891 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:49:01,891 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:49:01,891 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds."
"2023-09-19T08:49:01,901 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:49:01,901 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:49:01,902 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=924"
"2023-09-19T08:49:01,903 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:49:01,912 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:01,912 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]924"
"2023-09-19T08:49:01,912 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:01,912 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:01,912 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:49:01,913 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:01,913 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:49:01,913 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:01,913 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:49:01,913 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:49:01,913 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds."
"2023-09-19T08:49:01,928 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:49:01,928 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:49:02,103 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=934"
"2023-09-19T08:49:02,103 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:49:02,112 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:02,112 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]934"
"2023-09-19T08:49:02,113 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:02,113 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:02,113 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:49:02,113 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:02,113 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:49:02,113 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:02,114 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:49:02,114 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:49:02,114 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds."
"2023-09-19T08:49:02,115 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=937"
"2023-09-19T08:49:02,115 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:49:02,123 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:49:02,123 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:49:02,125 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:02,125 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]937"
"2023-09-19T08:49:02,125 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:02,125 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:02,125 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:49:02,126 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:02,126 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:49:02,126 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:02,126 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:49:02,126 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:49:02,126 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds."
"2023-09-19T08:49:02,136 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:49:02,136 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:49:05,494 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:05,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113345"
"2023-09-19T08:49:10,495 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:10,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113350"
"2023-09-19T08:49:15,495 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:49:15,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113355"
"2023-09-19T08:49:20,495 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:49:20,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113360"
"2023-09-19T08:49:25,495 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:25,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113365"
"2023-09-19T08:49:30,494 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:30,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113370"
"2023-09-19T08:49:34,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=985"
"2023-09-19T08:49:34,779 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:49:34,788 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:34,788 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]985"
"2023-09-19T08:49:34,788 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:34,788 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:34,788 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:49:34,789 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:34,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:49:34,789 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:34,789 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:49:34,789 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:49:34,789 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds."
"2023-09-19T08:49:34,799 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:49:34,799 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:49:35,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:35,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113375"
"2023-09-19T08:49:37,667 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=995"
"2023-09-19T08:49:37,668 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:49:37,684 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:37,685 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]995"
"2023-09-19T08:49:37,685 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:37,685 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:37,685 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:49:37,686 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:37,687 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:37,687 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:49:37,688 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:49:37,688 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 55 seconds."
"2023-09-19T08:49:37,688 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:49:37,698 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=998"
"2023-09-19T08:49:37,698 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:49:37,705 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:49:37,705 [ERROR] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:49:37,711 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:37,712 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]998"
"2023-09-19T08:49:37,712 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:37,712 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:37,712 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:49:37,713 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:37,713 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:49:37,713 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:37,713 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:49:37,713 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:49:37,713 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 55 seconds."
"2023-09-19T08:49:37,714 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:49:37,724 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:49:37,950 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=1001"
"2023-09-19T08:49:37,950 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:49:37,960 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:37,960 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]1001"
"2023-09-19T08:49:37,960 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:37,960 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:37,960 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:49:37,961 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:37,961 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:49:37,961 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:37,961 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:49:37,961 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:49:37,961 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds."
"2023-09-19T08:49:37,977 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:49:37,977 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:49:38,159 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=1004"
"2023-09-19T08:49:38,159 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:49:38,170 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:38,170 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]1004"
"2023-09-19T08:49:38,170 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:38,170 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:38,170 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:49:38,171 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:38,171 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:49:38,171 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:38,171 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:49:38,171 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:49:38,171 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds."
"2023-09-19T08:49:38,182 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:49:38,182 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:49:38,211 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=1007"
"2023-09-19T08:49:38,211 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:49:38,220 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:38,221 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]1007"
"2023-09-19T08:49:38,221 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:38,221 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:38,221 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:49:38,221 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:38,221 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:49:38,221 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:38,221 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=1015"
"2023-09-19T08:49:38,222 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:49:38,222 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:49:38,222 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:49:38,222 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds."
"2023-09-19T08:49:38,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=1010"
"2023-09-19T08:49:38,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:49:38,231 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:38,232 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]1015"
"2023-09-19T08:49:38,232 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:38,232 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:38,232 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:49:38,232 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:49:38,232 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:49:38,232 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:38,232 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:49:38,232 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:49:38,232 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]1010"
"2023-09-19T08:49:38,232 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:49:38,232 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:38,233 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:49:38,233 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:49:38,233 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:49:38,233 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:49:38,233 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds."
"2023-09-19T08:49:38,234 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:49:38,234 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:49:38,234 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:49:38,234 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:49:38,234 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:49:38,234 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds."
"2023-09-19T08:49:38,243 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:49:38,243 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:49:38,248 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:49:38,248 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:49:40,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:40,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113380"
"2023-09-19T08:49:45,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:49:45,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113385"
"2023-09-19T08:49:50,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:50,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113390"
"2023-09-19T08:49:55,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:49:55,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113395"
"2023-09-19T08:50:00,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:00,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113400"
"2023-09-19T08:50:05,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:05,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113405"
"2023-09-19T08:50:10,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:10,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113410"
"2023-09-19T08:50:15,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:15,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113415"
"2023-09-19T08:50:20,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:20,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113420"
"2023-09-19T08:50:25,494 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:25,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113425"
"2023-09-19T08:50:30,495 [INFO ] W-9005-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:50:30,495 [INFO ] W-9005-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113430"
"2023-09-19T08:50:31,150 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=1065"
"2023-09-19T08:50:31,151 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:50:31,160 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:31,160 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]1065"
"2023-09-19T08:50:31,160 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:31,160 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:31,160 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:50:31,160 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:31,160 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:50:31,161 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:31,161 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:50:31,161 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:50:31,161 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds."
"2023-09-19T08:50:31,172 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:50:31,172 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:50:34,623 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=1075"
"2023-09-19T08:50:34,624 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:50:34,640 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:34,640 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]1075"
"2023-09-19T08:50:34,640 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:34,641 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:34,641 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:50:34,642 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:50:34,643 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:34,643 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:34,644 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:50:34,644 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:50:34,644 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 89 seconds."
"2023-09-19T08:50:34,659 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:50:34,660 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:50:34,676 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=1078"
"2023-09-19T08:50:34,677 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:50:34,694 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:34,694 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]1078"
"2023-09-19T08:50:34,694 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:34,694 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:34,694 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:50:34,695 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:34,695 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:50:34,696 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:34,696 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:50:34,696 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:50:34,698 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 89 seconds."
"2023-09-19T08:50:34,711 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:50:34,711 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:50:35,106 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=1081"
"2023-09-19T08:50:35,107 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:50:35,123 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:35,124 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]1081"
"2023-09-19T08:50:35,124 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:35,124 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:35,124 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:50:35,125 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:35,125 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:50:35,125 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:35,125 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:50:35,125 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:50:35,125 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds."
"2023-09-19T08:50:35,140 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:50:35,141 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:50:35,213 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=1091"
"2023-09-19T08:50:35,214 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:50:35,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:35,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]1091"
"2023-09-19T08:50:35,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:35,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:35,223 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:50:35,224 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:35,224 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:50:35,224 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:35,224 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:50:35,224 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:50:35,224 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds."
"2023-09-19T08:50:35,229 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=1084"
"2023-09-19T08:50:35,229 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:50:35,238 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:35,239 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]1084"
"2023-09-19T08:50:35,239 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:35,239 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:35,239 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:50:35,239 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:35,239 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:50:35,239 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:50:35,239 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:35,239 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:50:35,242 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:50:35,242 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:50:35,242 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds."
"2023-09-19T08:50:35,250 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:50:35,250 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:50:35,257 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=1090"
"2023-09-19T08:50:35,257 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:50:35,266 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:35,266 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]1090"
"2023-09-19T08:50:35,266 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:35,266 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:35,266 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:50:35,267 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:35,267 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:50:35,267 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:35,267 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:50:35,267 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:50:35,267 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds."
"2023-09-19T08:50:35,277 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:50:35,277 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:50:35,309 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=1087"
"2023-09-19T08:50:35,309 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:50:35,318 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:50:35,318 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]1087"
"2023-09-19T08:50:35,318 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:50:35,318 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:50:35,318 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:50:35,319 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:50:35,319 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:50:35,319 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:50:35,319 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:50:35,319 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:50:35,319 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds."
"2023-09-19T08:50:35,329 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:50:35,329 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:50:35,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:35,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113435"
"2023-09-19T08:50:40,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:40,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113440"
"2023-09-19T08:50:45,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:45,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113445"
"2023-09-19T08:50:50,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:50,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113450"
"2023-09-19T08:50:55,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:50:55,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113455"
"2023-09-19T08:51:00,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:00,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113460"
"2023-09-19T08:51:05,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:05,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113465"
"2023-09-19T08:51:10,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:10,494 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113470"
"2023-09-19T08:51:15,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:15,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113475"
"2023-09-19T08:51:20,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:20,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113480"
"2023-09-19T08:51:25,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:25,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113485"
"2023-09-19T08:51:30,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:30,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113490"
"2023-09-19T08:51:35,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:35,494 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113495"
"2023-09-19T08:51:40,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:40,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113500"
"2023-09-19T08:51:45,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:45,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113505"
"2023-09-19T08:51:50,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:51:50,494 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113510"
"2023-09-19T08:51:55,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:51:55,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113515"
"2023-09-19T08:52:00,495 [INFO ] pool-2-thread-10 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:52:00,495 [INFO ] pool-2-thread-10 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113520"
"2023-09-19T08:52:01,525 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=1145"
"2023-09-19T08:52:01,525 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:52:01,534 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:01,534 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]1145"
"2023-09-19T08:52:01,535 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:01,535 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:01,535 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:52:01,535 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:01,535 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:52:01,535 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:01,536 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:52:01,536 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:52:01,536 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds."
"2023-09-19T08:52:01,546 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:52:01,546 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:52:05,458 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=1160"
"2023-09-19T08:52:05,459 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:52:05,493 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:05,494 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]1160"
"2023-09-19T08:52:05,494 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:05,495 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:52:05,495 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:05,496 [INFO ] pool-2-thread-16 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 1"
"2023-09-19T08:52:05,496 [INFO ] pool-2-thread-16 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113525"
"2023-09-19T08:52:05,498 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:05,499 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007."
"2023-09-19T08:52:05,499 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:05,499 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:52:05,499 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:52:05,500 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 144 seconds."
"2023-09-19T08:52:05,515 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:52:05,516 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:52:05,690 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=1157"
"2023-09-19T08:52:05,691 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:52:05,707 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:05,708 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]1157"
"2023-09-19T08:52:05,708 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:05,708 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:05,708 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:52:05,709 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:05,709 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:52:05,709 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:05,709 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:52:05,710 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:52:05,710 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 144 seconds."
"2023-09-19T08:52:05,725 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:52:05,725 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:52:06,038 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=1165"
"2023-09-19T08:52:06,039 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:52:06,048 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:06,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]1165"
"2023-09-19T08:52:06,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:06,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:06,049 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:52:06,050 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:06,050 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:52:06,050 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:06,051 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:52:06,051 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:52:06,051 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds."
"2023-09-19T08:52:06,061 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:52:06,061 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:52:06,230 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=1194"
"2023-09-19T08:52:06,231 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:52:06,240 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:06,240 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]1194"
"2023-09-19T08:52:06,240 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:06,240 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:06,240 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:52:06,241 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:06,241 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:52:06,242 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:06,242 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:52:06,242 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:52:06,242 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds."
"2023-09-19T08:52:06,252 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:52:06,252 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:52:06,301 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=1184"
"2023-09-19T08:52:06,301 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:52:06,310 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:06,310 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]1184"
"2023-09-19T08:52:06,310 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:06,310 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:06,310 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:52:06,311 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:06,311 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:52:06,312 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:06,322 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:52:06,322 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:52:06,322 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:52:06,322 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:52:06,323 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds."
"2023-09-19T08:52:06,399 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=1188"
"2023-09-19T08:52:06,399 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:52:06,408 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:06,408 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]1188"
"2023-09-19T08:52:06,408 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:06,408 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:06,408 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:52:06,409 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:06,409 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:52:06,409 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:06,409 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:52:06,409 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:52:06,410 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds."
"2023-09-19T08:52:06,419 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:52:06,419 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:52:06,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=1199"
"2023-09-19T08:52:06,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:52:06,486 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:52:06,486 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]1199"
"2023-09-19T08:52:06,486 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:52:06,487 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:52:06,487 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:52:06,487 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:52:06,487 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:52:06,488 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:52:06,488 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:52:06,488 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:52:06,488 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds."
"2023-09-19T08:52:06,498 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:52:06,498 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:52:10,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:10,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113530"
"2023-09-19T08:52:15,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:15,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113535"
"2023-09-19T08:52:20,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:20,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113540"
"2023-09-19T08:52:25,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:25,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113545"
"2023-09-19T08:52:30,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:30,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113550"
"2023-09-19T08:52:35,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:35,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113555"
"2023-09-19T08:52:40,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:40,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113560"
"2023-09-19T08:52:45,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 200 0"
"2023-09-19T08:52:45,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113565"
"2023-09-19T08:52:50,494 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:57528 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:52:50,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113570"
"2023-09-19T08:52:55,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:43142 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:52:55,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113575"
"2023-09-19T08:53:00,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:42396 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:53:00,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113580"
"2023-09-19T08:53:05,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:42410 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:53:05,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113585"
"2023-09-19T08:53:10,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:60016 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:53:10,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113590"
"2023-09-19T08:53:15,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:60026 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:53:15,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113595"
"2023-09-19T08:53:20,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:55434 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:53:20,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113600"
"2023-09-19T08:53:25,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:55440 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:53:25,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113605"
"2023-09-19T08:53:30,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:43302 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:53:30,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113610"
"2023-09-19T08:53:35,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:43316 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:53:35,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113615"
"2023-09-19T08:53:40,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:55086 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:53:40,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113620"
"2023-09-19T08:53:45,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:55094 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:53:45,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113625"
"2023-09-19T08:53:50,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:47580 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:53:50,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113630"
"2023-09-19T08:53:55,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:47594 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:53:55,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113635"
"2023-09-19T08:54:00,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:51032 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:54:00,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113640"
"2023-09-19T08:54:05,496 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:51042 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:54:05,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113645"
"2023-09-19T08:54:10,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:46376 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:10,496 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113650"
"2023-09-19T08:54:15,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:46384 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:15,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113655"
"2023-09-19T08:54:20,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:40332 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:20,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113660"
"2023-09-19T08:54:25,495 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.178.2:40342 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:25,495 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113665"
"2023-09-19T08:54:26,906 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=1254"
"2023-09-19T08:54:26,907 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:54:26,916 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:26,916 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]1254"
"2023-09-19T08:54:26,916 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:26,916 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:26,916 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001"
"2023-09-19T08:54:26,917 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:26,917 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001."
"2023-09-19T08:54:26,917 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:26,917 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr"
"2023-09-19T08:54:26,917 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout"
"2023-09-19T08:54:26,929 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout"
"2023-09-19T08:54:26,929 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr"
"2023-09-19T08:54:30,498 [INFO ] pool-2-thread-23 ACCESS_LOG - /169.254.178.2:59190 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:54:30,499 [INFO ] pool-2-thread-23 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113670"
"2023-09-19T08:54:31,275 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9007, pid=1265"
"2023-09-19T08:54:31,275 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:54:31,308 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:31,352 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]1265"
"2023-09-19T08:54:31,354 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:31,354 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007"
"2023-09-19T08:54:31,354 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:31,354 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:31,355 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:31,356 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stderr"
"2023-09-19T08:54:31,356 [WARN ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-model_1.0-stdout"
"2023-09-19T08:54:31,374 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception"
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
"2023-09-19T08:54:31,375 [INFO ] W-9007-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stderr"
"2023-09-19T08:54:31,375 [INFO ] W-9007-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-model_1.0-stdout"
"2023-09-19T08:54:31,426 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9006, pid=1269"
"2023-09-19T08:54:31,426 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:54:31,443 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:31,443 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]1269"
"2023-09-19T08:54:31,443 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:31,443 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:31,443 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006"
"2023-09-19T08:54:31,444 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:31,444 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006."
"2023-09-19T08:54:31,444 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:31,444 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr"
"2023-09-19T08:54:31,445 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout"
"2023-09-19T08:54:31,460 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr"
"2023-09-19T08:54:31,461 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout"
"2023-09-19T08:54:32,074 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=1281"
"2023-09-19T08:54:32,074 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:54:32,084 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:32,084 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]1281"
"2023-09-19T08:54:32,084 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:32,084 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:32,084 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000"
"2023-09-19T08:54:32,085 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:32,085 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000."
"2023-09-19T08:54:32,085 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:32,085 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr"
"2023-09-19T08:54:32,085 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout"
"2023-09-19T08:54:32,095 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout"
"2023-09-19T08:54:32,095 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr"
"2023-09-19T08:54:32,260 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9005, pid=1286"
"2023-09-19T08:54:32,261 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:54:32,270 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:32,270 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]1286"
"2023-09-19T08:54:32,270 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:32,270 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:32,271 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005"
"2023-09-19T08:54:32,271 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:32,271 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005."
"2023-09-19T08:54:32,272 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:32,272 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stderr"
"2023-09-19T08:54:32,272 [WARN ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-model_1.0-stdout"
"2023-09-19T08:54:32,282 [INFO ] W-9005-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stdout"
"2023-09-19T08:54:32,282 [INFO ] W-9005-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-model_1.0-stderr"
"2023-09-19T08:54:32,369 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=1308"
"2023-09-19T08:54:32,369 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:54:32,379 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:32,379 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]1308"
"2023-09-19T08:54:32,379 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:32,379 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:32,379 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003"
"2023-09-19T08:54:32,380 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:32,380 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003."
"2023-09-19T08:54:32,380 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:32,380 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr"
"2023-09-19T08:54:32,380 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout"
"2023-09-19T08:54:32,390 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout"
"2023-09-19T08:54:32,390 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr"
"2023-09-19T08:54:32,485 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9004, pid=1303"
"2023-09-19T08:54:32,485 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:54:32,487 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=1298"
"2023-09-19T08:54:32,487 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:54:32,495 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:32,495 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]1303"
"2023-09-19T08:54:32,495 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:32,495 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:32,495 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004"
"2023-09-19T08:54:32,496 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:32,496 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004."
"2023-09-19T08:54:32,496 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:32,496 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stderr"
"2023-09-19T08:54:32,496 [WARN ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1.0-stdout"
"2023-09-19T08:54:32,496 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml."
"2023-09-19T08:54:32,497 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]1298"
"2023-09-19T08:54:32,497 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started."
"2023-09-19T08:54:32,497 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9"
"2023-09-19T08:54:32,497 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002"
"2023-09-19T08:54:32,497 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED"
"2023-09-19T08:54:32,497 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002."
"2023-09-19T08:54:32,498 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again"
"2023-09-19T08:54:32,498 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr"
"2023-09-19T08:54:32,498 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout"
"2023-09-19T08:54:32,511 [INFO ] W-9004-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stdout"
"2023-09-19T08:54:32,511 [INFO ] W-9004-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1.0-stderr"
"2023-09-19T08:54:32,513 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout"
"2023-09-19T08:54:32,513 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr"
"2023-09-19T08:54:35,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:59192 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:35,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113675"
"2023-09-19T08:54:40,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:51140 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:40,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113680"
"2023-09-19T08:54:45,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:51148 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:45,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113685"
"2023-09-19T08:54:50,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:47712 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:50,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113690"
"2023-09-19T08:54:55,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:47724 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:54:55,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113695"
"2023-09-19T08:55:00,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:48236 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:00,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113700"
"2023-09-19T08:55:05,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:48250 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:05,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113705"
"2023-09-19T08:55:10,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:40182 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:10,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113710"
"2023-09-19T08:55:15,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:40192 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:15,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113715"
"2023-09-19T08:55:20,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:60870 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:20,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113720"
"2023-09-19T08:55:25,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:60882 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:25,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113725"
"2023-09-19T08:55:30,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:59288 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:30,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113730"
"2023-09-19T08:55:35,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:59296 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:35,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113735"
"2023-09-19T08:55:40,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:51526 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:40,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113740"
"2023-09-19T08:55:45,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:51542 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:45,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113745"
"2023-09-19T08:55:50,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:35986 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:50,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113750"
"2023-09-19T08:55:55,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:35996 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:55:55,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113755"
"2023-09-19T08:56:00,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:33574 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:00,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113760"
"2023-09-19T08:56:05,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:33578 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:05,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113765"
"2023-09-19T08:56:10,496 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:40928 ""GET /ping HTTP/1.1"" 500 1"
"2023-09-19T08:56:10,496 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113770"
"2023-09-19T08:56:15,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:40942 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:15,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113775"
"2023-09-19T08:56:20,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:46300 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:20,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113780"
"2023-09-19T08:56:25,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:46310 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:25,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113785"
"2023-09-19T08:56:30,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:46352 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:30,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113790"
"2023-09-19T08:56:35,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:46358 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:35,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113795"
"2023-09-19T08:56:40,495 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.178.2:52012 ""GET /ping HTTP/1.1"" 500 0"
"2023-09-19T08:56:40,495 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:container-0.local,timestamp:1695113800"
